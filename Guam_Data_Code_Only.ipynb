{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHM/OtZAfhBALZmuYeuW4L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevenandreu/Guam-Data/blob/main/Guam_Data_Code_Only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Cn1sLGEOlXw"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/google-research/perch.git@373253f5887e2964a51f348e107889dadfdcece0\n",
        "\n",
        "# The necessary pipeline to connect this Colab environment with the Google Drive\n",
        "# folder where we host the data used in this tutorial.\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Create the folder to store the sample data\n",
        "drive_sample_data_folder = '/content/drive/MyDrive/SurfPerch Guam Data/'\n",
        "if not os.path.exists(drive_sample_data_folder):\n",
        "  os.mkdir(drive_sample_data_folder)\n",
        "\n",
        "# This is the location that this tutorial will use to save data.\n",
        "drive_output_directory = '/content/drive/MyDrive/SurfPerch Guam Output/'\n",
        "if not os.path.exists(drive_output_directory):\n",
        "  os.mkdir(drive_output_directory)\n",
        "\n",
        "  # Import various dependencies, including the relevant modules from the Perch\n",
        "# repository. Note that \"chirp\" is the old name that the Perch team used, so any\n",
        "# chirp modules imported here were installed as part of the Perch repository in\n",
        "# one of the previous cells.\n",
        "\n",
        "import collections\n",
        "from collections import Counter\n",
        "from etils import epath\n",
        "from IPython.display import HTML\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display as ipy_display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "from ml_collections import config_dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import wavfile\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from chirp.inference import colab_utils\n",
        "colab_utils.initialize(use_tf_gpu=True, disable_warnings=True)\n",
        "\n",
        "from chirp import audio_utils\n",
        "from chirp import config_utils\n",
        "from chirp import path_utils\n",
        "from chirp.inference import embed_lib\n",
        "from chirp.inference import models\n",
        "from chirp.inference import tf_examples\n",
        "from chirp.models import metrics\n",
        "from chirp.inference.search import bootstrap\n",
        "from chirp.inference.search import search\n",
        "from chirp.inference.search import display\n",
        "from chirp.inference.classify import classify\n",
        "from chirp.inference.classify import data_lib\n",
        "\n",
        "# If connected to a Colab GPU runtime we should see a GPU listed\n",
        "tf.config.list_physical_devices()\n",
        "\n",
        "# Model specific parameters: PLEASE DO NOT CHANGE THE CODE IN THIS CELL.\n",
        "config = config_dict.ConfigDict()\n",
        "embed_fn_config = config_dict.ConfigDict()\n",
        "embed_fn_config.model_key = 'taxonomy_model_tf'\n",
        "model_config = config_dict.ConfigDict()\n",
        "\n",
        "# The size of each \"chunk\" of audio.\n",
        "model_config.window_size_s = 5.0\n",
        "\n",
        "# The hop size\n",
        "model_config.hop_size_s = 5.0\n",
        "\n",
        "# All audio in this tutorial is resampled to 32 kHz.\n",
        "model_config.sample_rate = 32000\n",
        "\n",
        "# The location of the pre-trained model.\n",
        "model_config.model_path = drive_sample_data_folder + 'SurfPerch-model/'\n",
        "\n",
        "# Only write embeddings to reduce size. The Perch codebase supports serializing\n",
        "# a variety of metadata along with the embeddings, but for the purposes of this\n",
        "# tutorial we will not need to make use of those features.\n",
        "embed_fn_config.write_embeddings = True\n",
        "embed_fn_config.write_logits = False\n",
        "embed_fn_config.write_separated_audio = False\n",
        "embed_fn_config.write_raw_audio = False\n",
        "\n",
        "config.embed_fn_config = embed_fn_config\n",
        "embed_fn_config.model_config = model_config\n",
        "\n",
        "# These two settings can be used to break large inputs up into smaller chunks;\n",
        "# this is especially helpful for dealing with long files or very large datasets.\n",
        "# Given free colab has limited resources, you may want to reduce shard_len_s to\n",
        "# 10 to prevent system RAM from becoming overloaded.\n",
        "config.shard_len_s = 60 #\n",
        "config.num_shards_per_file = -1\n",
        "\n",
        "# Number of parent directories to include in the filename. This allows us to\n",
        "# process raw audio that lives in multiple directories.\n",
        "config.embed_fn_config.file_id_depth = 1\n",
        "\n",
        "# If your dataset is large its useful to split the TFRecords across multiple\n",
        "# shards so I/O operations can be parallized.\n",
        "config.tf_record_shards = 10\n",
        "\n",
        "# Specify a glob pattern matching any number of wave files.\n",
        "# Use [wW][aA][vV] to match .wav or .WAV files\n",
        "unlabeled_audio_pattern = os.path.join(drive_sample_data_folder, 'Ch1_Guam_Unf Day 2/*.[wW][aA][vV]')\n",
        "\n",
        "# Specify a directory where the embeddings will be written.\n",
        "embedding_output_dir = os.path.join(drive_output_directory, 'raw_embeddings/')\n",
        "if not os.path.exists(embedding_output_dir):\n",
        "  os.makedirs(embedding_output_dir, exist_ok=True)\n",
        "\n",
        "config.output_dir = embedding_output_dir\n",
        "config.source_file_patterns = [unlabeled_audio_pattern]\n",
        "\n",
        "# Create output directory and write the configuration.\n",
        "output_dir = epath.Path(config.output_dir)\n",
        "output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Don't forget to run the dropdown cell above!\n",
        "\n",
        "# This dumps a config json file next to the embeddings that allows us to reuse\n",
        "# the same embeddings and ensure that we have the correct config that was used\n",
        "# to generate them.\n",
        "embed_lib.maybe_write_config(config, output_dir)\n",
        "\n",
        "# Create SourceInfos configuration, used in sharded computation when computing\n",
        "# embeddings. These source_infos contain metadata about how we're going to\n",
        "# partition the search corpus.  In particular, we're splitting the Powdermill\n",
        "# audio into hundreds of 5s chunks, and the source_infos help us keep track of\n",
        "# which chunk came from which raw audio file.\n",
        "source_infos = embed_lib.create_source_infos(\n",
        "    config.source_file_patterns,\n",
        "    config.num_shards_per_file,\n",
        "    config.shard_len_s)\n",
        "print(f'Constructed {len(source_infos)} source infos.')\n",
        "\n",
        "#@title { vertical-output: true }\n",
        "# Here we're loading our generic Bird Classifier model.\n",
        "# The embed_fn object is a wrapper over the model.\n",
        "embed_fn = embed_lib.EmbedFn(**config.embed_fn_config)\n",
        "print('\\n\\nLoading model(s)...')\n",
        "embed_fn.setup()\n",
        "\n",
        "print('\\n\\nTest-run of model...')\n",
        "z = np.zeros([int(model_config.sample_rate * model_config.window_size_s)])\n",
        "embed_fn.embedding_model.embed(z)\n",
        "print('Setup complete!')\n",
        "\n",
        "# To reduce the overhead computational resources required and speed up execution\n",
        "# time, we use multiple threads to load the audio before embedding. This tends\n",
        "# to perform faster, but can fail if any audio files are corrupt.\n",
        "\n",
        "# The source_infos variable contains metadata about how to parition the search\n",
        "# corpus.  This step creates an audio_iterator which iterates over the 5 second\n",
        "# chunks of audio.\n",
        "\n",
        "embed_fn.min_audio_s = 1.0\n",
        "record_file = (output_dir / 'embeddings.tfrecord').as_posix()\n",
        "succ, fail = 0, 0\n",
        "\n",
        "audio_loader = lambda fp, offset: audio_utils.load_audio_window(\n",
        "    fp, offset, model_config.sample_rate, config.shard_len_s)\n",
        "audio_iterator = audio_utils.multi_load_audio_window(\n",
        "    audio_loader=audio_loader,\n",
        "    filepaths=[s.filepath for s in source_infos],\n",
        "    offsets=[s.shard_num * s.shard_len_s for s in source_infos],\n",
        ")\n",
        "\n",
        "#@title { vertical-output: true }\n",
        "# Embed! This step may take several minutes to run.\n",
        "with tf_examples.EmbeddingsTFRecordMultiWriter(\n",
        "    output_dir=output_dir, num_files=config.tf_record_shards) as file_writer:\n",
        "  for source_info, audio in tqdm.tqdm(\n",
        "      zip(source_infos, audio_iterator), total=len(source_infos)):\n",
        "    if audio.shape[0] < embed_fn.min_audio_s * model_config.sample_rate:\n",
        "      # Ignore short audio.\n",
        "      continue\n",
        "    file_id = source_info.file_id(config.embed_fn_config.file_id_depth)\n",
        "    offset_s = source_info.shard_num * source_info.shard_len_s\n",
        "    example = embed_fn.audio_to_example(file_id, offset_s, audio)\n",
        "    if example is None:\n",
        "      fail += 1\n",
        "      continue\n",
        "    file_writer.write(example.SerializeToString())\n",
        "    succ += 1\n",
        "  file_writer.flush()\n",
        "print(f'\\n\\nSuccessfully processed {succ} source_infos, failed {fail} times.')\n",
        "\n",
        "fns = [fn for fn in output_dir.glob('embeddings-*')]\n",
        "ds = tf.data.TFRecordDataset(fns)\n",
        "parser = tf_examples.get_example_parser()\n",
        "ds = ds.map(parser)\n",
        "for ex in ds.as_numpy_iterator():\n",
        "  print('Recording filename:', ex['filename'])\n",
        "  print('Shape of the embedding:', ex['embedding'].shape)\n",
        "  break\n",
        "\n",
        "# This can take a few moments to get started"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { vertical-output: true }\n",
        "# Path to cioc target sound folders\n",
        "cioc_sounds = drive_sample_data_folder + '/Holo'\n",
        "cioc_sounds_folders = os.listdir(cioc_sounds)\n",
        "\n",
        "# For each target sound folder, find the first audio file as an example\n",
        "example_target_sounds = []\n",
        "for folder in cioc_sounds_folders:\n",
        "  wav_files = [file for file in os.listdir(os.path.join(cioc_sounds, folder)) if file.lower().endswith('.wav')]\n",
        "  example_sound_path = os.path.join(cioc_sounds, folder + '/' + wav_files[0])\n",
        "  example_target_sounds.append(example_sound_path)\n",
        "\n",
        "# Now view each example target sound\n",
        "print('Number of different target sounds: ', len(example_target_sounds))\n",
        "for audio_path in example_target_sounds:\n",
        "  print('Target sound label: ', audio_path.split('/')[-2])\n",
        "  audio = audio_utils.load_audio(audio_path, model_config.sample_rate)\n",
        "  display.plot_audio_melspec(audio, model_config.sample_rate)"
      ],
      "metadata": {
        "id": "65d3RZ3rPYXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Hit run on this cell and pick a target sound\n",
        "\n",
        "# Ensure the path exists and list directories\n",
        "if os.path.exists(cioc_sounds):\n",
        "    sound_folders = [f for f in os.listdir(cioc_sounds) if os.path.isdir(os.path.join(cioc_sounds, f))]\n",
        "else:\n",
        "    print(\"Path does not exist:\", cioc_sounds)\n",
        "    sound_folders = []\n",
        "\n",
        "# Create and display the dropdown\n",
        "sound_dropdown = widgets.Dropdown(\n",
        "    options=sound_folders,\n",
        "    description='Select sound:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "# Define a function that reacts to changes in the dropdown\n",
        "def on_sound_change(change):\n",
        "    choice = change['new']\n",
        "    print(f'Changed target sound to: {choice}. Now work through the cells below for this dataset.')\n",
        "\n",
        "# Attach the observer to the dropdown\n",
        "sound_dropdown.observe(on_sound_change, names='value')\n",
        "\n",
        "ipy_display(sound_dropdown)"
      ],
      "metadata": {
        "id": "hrcW9RN7Pgbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load and view the query audio sample { vertical-output: true }\n",
        "target_sound = sound_dropdown.value\n",
        "target_classes = [target_sound, 'Unknown']\n",
        "\n",
        "# Select one of the target audio files. Default with 1, but for many sounds\n",
        "# CIOC users surfaced multiple copies which we can leverage. The print\n",
        "# out from this cell will tell you if there are others to choose from.\n",
        "file_index = 1  #@param\n",
        "file_index = file_index -1\n",
        "\n",
        "# Build the folder path\n",
        "target_audio_folder = os.path.join(drive_sample_data_folder, 'Holo', target_sound)\n",
        "\n",
        "# Retrieve all .wav files\n",
        "wav_files = [file for file in os.listdir(target_audio_folder) if file.lower().endswith('.wav')]\n",
        "\n",
        "# Print the total number of audio files\n",
        "print(f\"Number of indexed audio files in target sound directory: {len(wav_files)}\")\n",
        "\n",
        "# Validate the user input and select the audio file\n",
        "if 0 <= file_index < len(wav_files):\n",
        "    audio_path = os.path.join(target_audio_folder, wav_files[file_index])\n",
        "    print(f\"Viewing example: {file_index + 1}\")\n",
        "else:\n",
        "    print(\"Invalid file index. Please select a valid index up to and including: \", len(wav_files))\n",
        "    audio_path = None\n",
        "\n",
        "# Assuming the rest of the code executes only if a valid path is selected\n",
        "if audio_path:\n",
        "    audio = audio_utils.load_audio(audio_path, model_config.sample_rate)\n",
        "    display.plot_audio_melspec(audio, model_config.sample_rate)\n",
        ""
      ],
      "metadata": {
        "id": "teVkboNtPj5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { vertical-output: true }\n",
        "# If you're audio clip is longer than 5s, adjust start_s to pick your\n",
        "# prefered start time.\n",
        "start_s = 0  #@param\n",
        "\n",
        "# Display the selected window.\n",
        "print('Selected audio window:')\n",
        "st = int(start_s * model_config.sample_rate)\n",
        "end = int(st + model_config.window_size_s * model_config.sample_rate)\n",
        "if end > audio.shape[0]:\n",
        "  end = audio.shape[0]\n",
        "  st = max([0, int(end - model_config.window_size_s * model_config.sample_rate)])\n",
        "audio_window = audio[st:end]\n",
        "display.plot_audio_melspec(audio_window, model_config.sample_rate)\n",
        "\n",
        "query_audio = audio_window\n",
        "sep_outputs = None"
      ],
      "metadata": {
        "id": "Xw7IxJF3PnvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The path to an empty directory where the generated labeled samples will be\n",
        "# placed. Each labeled sample will be placed into a subdirectory corresponding\n",
        "# to the target class that we select for that sample.\n",
        "target_audio_outputs = os.path.join(drive_output_directory, target_sound + '/labeled_outputs/' + target_sound)\n",
        "os.makedirs(target_audio_outputs, exist_ok=True)\n",
        "\n",
        "# Copy all .wav and .WAV files from target_audio_folder to labeled_data_path\n",
        "for file in os.listdir(target_audio_folder):\n",
        "    if file.lower().endswith('.wav'):\n",
        "        source_path = os.path.join(target_audio_folder, file)\n",
        "        destination_path = os.path.join(target_audio_outputs, file)\n",
        "        shutil.copy2(source_path, destination_path)\n",
        "\n",
        "query = query_audio\n",
        "\n",
        "embedded_query = embed_fn.embedding_model.embed(query).embeddings[ :, 0, :]\n",
        "\n",
        "# Use the embedded dataset that we created above...\n",
        "bootstrap_config = bootstrap.BootstrapConfig.load_from_embedding_path(\n",
        "    embeddings_path=embedding_output_dir,\n",
        "    annotated_path=target_audio_outputs\n",
        ")\n",
        "\n",
        "project_state = bootstrap.BootstrapState(\n",
        "    bootstrap_config, embedding_model=embed_fn.embedding_model)\n",
        "\n",
        "embeddings_ds = project_state.create_embeddings_dataset()"
      ],
      "metadata": {
        "id": "m3tj7QHZPrev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of search results to capture. top_k = 25 is often a good start,\n",
        "# but we use 10 for brevity in this demo.\n",
        "top_k = 73 # @param {type:\"number\"}\n",
        "\n",
        "# The Perch codebase supports:\n",
        "#  'euclidean', which is the standard euclidean distance\n",
        "#  'cosine', which is the cosine similarity,\n",
        "#  'mip', which is Maximum Inner Product\n",
        "metric = 'euclidean'  #@param['euclidean', 'mip', 'cosine']\n",
        "\n",
        "# Target distance for search results. This lets us try to hone in on a\n",
        "# 'classifier boundary' instead of just looking at the closest matches.\n",
        "# Set to 'None' for raw 'best results' search.\n",
        "target_score = None #@param\n",
        "\n",
        "results, all_scores = search.search_embeddings_parallel(\n",
        "    embeddings_ds, embedded_query,\n",
        "    hop_size_s=model_config.hop_size_s,\n",
        "    top_k=top_k, target_score=target_score, score_fn=metric,\n",
        "    random_sample=False)"
      ],
      "metadata": {
        "id": "hjs1j9ntQCF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display the search results for user labeling. { vertical-output: true }\n",
        "display.display_search_results(\n",
        "    project_state=project_state,\n",
        "    results=results,\n",
        "    embedding_sample_rate=model_config.sample_rate,\n",
        "    checkbox_labels=target_classes,\n",
        "    max_workers=5)\n",
        "\n",
        "# Let this cell finish executing before labeling the samples"
      ],
      "metadata": {
        "id": "W2ILrWeyQfGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { vertical-output: true }\n",
        "# Plot histogram of distances.\n",
        "ys, _, _ = plt.hist(all_scores, bins=128, density=True)\n",
        "hit_scores = [r.score for r in results.search_results]\n",
        "plt.scatter(hit_scores, np.zeros_like(hit_scores), marker='|',\n",
        "            color='r', alpha=0.5)\n",
        "\n",
        "plt.xlabel(metric)\n",
        "plt.ylabel('density')\n",
        "if target_score is not None:\n",
        "  plt.plot([target_score, target_score], [0.0, np.max(ys)], 'r:')\n",
        "  # Compute the proportion of scores < target_score.\n",
        "  hit_percentage = (all_scores < target_score).mean()\n",
        "  print(f'score < target_score percentage : {hit_percentage:5.3f}')\n",
        "min_score = np.min(all_scores)\n",
        "plt.plot([min_score, min_score], [0.0, np.max(ys)], 'g:')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jbimPCOdQieB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_labeled_data(search_results, labeled_data_path: str, sample_rate: int):\n",
        "  \"\"\"Write labeled results to the labeled data collection.\"\"\"\n",
        "  labeled_data_path = epath.Path(labeled_data_path)\n",
        "  counts = collections.defaultdict(int)\n",
        "  duplicates = collections.defaultdict(int)\n",
        "  for r in search_results:\n",
        "    labels = [ch.description for ch in r.label_widgets if ch.value]\n",
        "    if not labels:\n",
        "      continue\n",
        "    extension = epath.Path(r.filename).suffix\n",
        "    filename = epath.Path(r.filename).name[: -len(extension)]\n",
        "    output_filename = f'{filename}___{r.timestamp_offset}{extension}'\n",
        "    for label in labels:\n",
        "      output_path = labeled_data_path / label\n",
        "      output_path.mkdir(parents=True, exist_ok=True)\n",
        "      output_filepath = epath.Path(output_path / output_filename)\n",
        "      if output_filepath.exists():\n",
        "        duplicates[f'{label}'] += 1\n",
        "        continue\n",
        "      else:\n",
        "        counts[label] += 1\n",
        "      with output_filepath.open('wb') as f:\n",
        "        wavfile.write(f, sample_rate, np.float32(r.audio))\n",
        "  for label, count in counts.items():\n",
        "    print(f'Wrote {count} examples for label {label}')\n",
        "  for label, count in duplicates.items():\n",
        "    print(f'Not saving {count} duplicates for label {label}')\n",
        "\n",
        "labeled_data_path = os.path.join(drive_output_directory, target_sound + '/labeled_outputs/')\n",
        "write_labeled_data(results, labeled_data_path, model_config.sample_rate)"
      ],
      "metadata": {
        "id": "BT23Z-PkQwcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load and embed the search-annotated dataset { vertical-output: true }\n",
        "\n",
        "# Load the training data that is located in the `labeled_data_path` directory.\n",
        "# In that directory there will be folders corresponding to our target labels\n",
        "\n",
        "merged = data_lib.MergedDataset.from_folder_of_folders(\n",
        "    base_dir=labeled_data_path,\n",
        "    embedding_model=project_state.embedding_model,\n",
        "    time_pooling='mean',\n",
        "    load_audio=False,\n",
        "    target_sample_rate=-2,\n",
        "    audio_file_pattern='*',\n",
        "    embedding_config_hash=bootstrap_config.embedding_config_hash(),\n",
        ")\n",
        "\n",
        "# Label distribution\n",
        "lbl_counts = np.sum(merged.data['label_hot'], axis=0)\n",
        "print('num classes :', (lbl_counts > 0).sum())\n",
        "print('mean ex / class :', lbl_counts.sum() / (lbl_counts > 0).sum())\n",
        "print('min ex / class :', (lbl_counts + (lbl_counts == 0) * 1e6).min())"
      ],
      "metadata": {
        "id": "iP9Pf9voQ4Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of random training examples to choose from each class.\n",
        "\n",
        "# Note that if you don't have very many samples you'll need to set\n",
        "# train_ratio=None and train_examples_per_class to a value that is\n",
        "# less than the minimum number of examples you have of each class.\n",
        "\n",
        "# Set exactly one of train_ratio and train_examples_per_class\n",
        "train_ratio = 1  #@param\n",
        "train_examples_per_class = None  #@param\n",
        "\n",
        "# Number of random re-trainings. In other words, this value indicates how many\n",
        "# models we will train, each will use a new randomly selected combination of\n",
        "# our labeled samples for training and testing. By training multiple models,\n",
        "# we get a sense of model robustness. Here, we train 3, but feel free to\n",
        "# increase it for added confidence in the model's performance.\n",
        "num_seeds = 3  #@param\n",
        "\n",
        "# Classifier training hyperparams.\n",
        "# These should be good defaults.\n",
        "batch_size = 12\n",
        "num_epochs = 128\n",
        "num_hiddens = -1\n",
        "learning_rate = 1e-3"
      ],
      "metadata": {
        "id": "iJFbKetIRHNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### WHERE I GOT STOPPED\n",
        "\n",
        "\n",
        "\n",
        "# This cell trains the linear model(s) and outputs some summary statistics for\n",
        "# each model. If you only have num_seeds = 1 then we'll only train a single\n",
        "# model here.\n",
        "metrics = collections.defaultdict(list)\n",
        "for seed in tqdm.tqdm(range(num_seeds)):\n",
        "  if num_hiddens > 0:\n",
        "    model = classify.get_two_layer_model(\n",
        "        num_hiddens, merged.embedding_dim, merged.num_classes)\n",
        "  else:\n",
        "    model = classify.get_linear_model(\n",
        "        merged.embedding_dim, merged.num_classes)\n",
        "  # Explicitly cast train_ratio to int to ensure integer indices for splitting\n",
        "  # This assumes train_ratio represents the number of training samples per class.\n",
        "  # If it's supposed to be a proportion, more changes are required within the library\n",
        "  # to ensure integer indices are generated.\n",
        "\n",
        "  run_metrics = classify.train_embedding_model(\n",
        "      model, merged, int(train_ratio), train_examples_per_class,\n",
        "      num_epochs, seed, batch_size, learning_rate)\n",
        "  metrics['acc'].append(run_metrics.top1_accuracy)\n",
        "  metrics['auc_roc'].append(run_metrics.auc_roc)\n",
        "  metrics['cmap'].append(run_metrics.cmap_value)\n",
        "  metrics['maps'].append(run_metrics.class_maps)\n",
        "  metrics['test_logits'].append(run_metrics.test_logits)"
      ],
      "metadata": {
        "id": "UXsZXJV7RH8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}